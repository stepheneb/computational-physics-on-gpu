A Comprehensive Theory of Volumetric Radiance Estimation Using Photon Points and Beams

WOJCIECH JAROSZ Disney Research Zurich and University of California San Diego 
DEREK NOWROUZEZAHRAI Disney Research Zurich and University of Toronto and 
IMAN SADEGHI and HENRIK WANN JENSEN University of California San Diego 

We present two contributions to the area of volumetric rendering. We develop a novel, comprehensive theory of volumetric radiance estimation that leads to several new insights and includes all previously published estimates as special cases. This theory allows for estimating in-scattered radiance at a point, or accumulated radiance along a camera ray, with the standard photon particle representation used in previous work. Furthermore, we generalize these operations to include a more compact, and more expressive intermediate representation of lighting in participating media, which we call œ photon beams. The combination of these representations and their respective query operations results in a collection of nine distinct volumetric radiance estimates. 

Our second contribution is a more ef cient rendering method for participating media based on photon beams. Even when shooting and storing less photons and using less computation time, our method signicantly reduces both bias (blur) and variance in volumetric radiance estimation. This enables us to render sharp lighting details (e.g., volume caustics) using just tens of thousands of photon beams, instead of the millions to billions of photon points required with previous methods. 

Categories and Subject Descriptors: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism ”Color, shading, shadowing, and texture; raytracing; I.6.8 [Simulation and Modeling]: Types of Simulation ”Monte Carlo; G.3 [Mathematics of Computing]: Probability and Statistics ”Probabilistic algorithms (including Monte Carlo) 

General Terms: Theory, Algorithms, Performance 

Additional Key Words and Phrases: Global illumination, ray marching, rendering, density estimation, photon map, particle tracing, participating media 

ACM Reference Format: 

Jarosz, W., Nowrouzezahrai, D., Sadeghi, I., and Jensen, H. W. 2011. A comprehensive theory of volumetric radiance estimation using photon points and beams. ACM Trans. Graph. 30, 1, Article 5 (January 2011), 19 pages. DOI = 10.1145/1899404.1899409 http://doi.acm.org/10.1145/1899404.1899409 

1. INTRODUCTION 

Participating media is responsible for some of the most visually compelling effects we see in the world. The appearance of re, water, smoke, clouds, rainbows, crepuscular œgod rays, and all organic materials is due to the way these media œparticipate in light interactions by emitting, absorbing, or scattering photons. These phenomena are common in the real world but, unfortunately, are incredibly costly to simulate accurately. Because of this, computer graphics has had a long-standing interest in developing more ef cient, accurate, and general participating media rendering tech- niques. We refer the reader to the recent survey by Cerezo et al. [2005] for a comprehensive overview. The most general techniques often use a form of stochastic sampling and Monte Carlo integration. This includes unbiased techniques such as (bidirectional) path tracing [Lafortune and Willems 1993, 1996, Veach and Guibas 1994] or Metropolis light transport [Pauly et al. 2000]; however, the most successful approaches typically rely on biased Monte Carlo combined with photon tracing [Keller 1997; Jensen and Christensen 1998; Walter et al. 2006; Jarosz et al. 2008]. Like bidirectional path tracing, photon tracing methods generate both camera and light paths but, instead of  ¢ W. Jarosz et al. estimates. 

We demonstrate their effectiveness on a number of scenes, and also discuss computational considerations, such as ef cient data structures, to make photon beams practical. Lastly, our uni ed theory provides new insights into how seemingly distinct volume rendering methods are in fact extremely similar. In addition to volumetric photon mapping, we show how virtual point-light methods [Keller 1997; Walter et al. 2006], light beam tracing methods [Nishita et al. 1987; Watt 1990; Nishita and Nakamae 1994], and deep shadow maps [Lokovic and Veach 2000] can all be seen as special cases or slight modi cations of our theory. 

2. Fig. 1. Volumetric photon mapping (left) stores scattering events at points (green) and performs density estimation. With photon beams (right), the full trajectory (lines) of each photon is stored and density estimation is performed on line segments. Photon beams increase the quality of radiance estimation since the space is lled more densely (e.g., the blue search region does not overlap any photon points, but does overlap two photon beams). RELATED WORK Our work is related to a number of previous techniques for coupling light paths to camera paths in participating media. 2.1 Photon Mapping coupling these two types of paths directly, they trace and store a collection of paths from the lights rst, and then determine a way to couple these light paths with the camera paths generated during rendering. Volumetric photon mapping [Jensen and Christensen 1998; Jarosz et al. 2008] performs density estimation on the vertices of these paths (the œphotons ) to estimate volumetric radiance. This process is energy preserving, but blurs the results, introducing bias. However, this bias reduces noise and allows for ef cient simulation of a wider range of light transport paths, such as caustics. 1.1 Motivation One of the primary motivations for our work is that current photon tracing methods for participating media are limited by the data representation used to store light paths. Current methods use a photon particle representation that only retains information about the scattering event locations, discarding all other potentially important information accumulated during photon tracing. We observe that by retaining more information about the light paths during photon tracing, we can obtain vastly improved rendering results, as illustrated in Figure 1. In this example, retaining only light path vertices (photon points) results in a sparse sampling of the light eld which, for density estimation techniques, either requires a large search radius with high bias or results in no photons being found within a xed radius (highlighted in blue). In contrast, if we store full light paths (photon beams) and an approach for computing density estimation using these paths existed, the density of data would be implicitly higher. These benefits motivate the main contributions of our work. We use volumetric photon mapping as the foundation for deriving a novel density estimation framework using points and beams. The original algorithm [Jensen and Christensen 1998] introduced a volumetric radiance estimate to approximate the in-scattered radiance at any discrete point in the medium. Radiance towards the eye is accumulated using ray marching, aggregating photon map queries along camera rays. Jarosz et al. [2008] target the inef ciencies of ray marching, formulating a new volumetric œbeam radiance estimate to consider all photons around the length of a camera ray (the œbeam ) in a single query. Boudet et al. [2005] and Schj th [2009] derived a photon splatting procedure which is mathematically equivalent for camera rays. These estimates change the query de nition from a point to a beam. In our work, we develop the tools necessary to change the data representation from a photon point to a photon beam. Moreover, we show how to couple any combination of point- or beam-query with a point- or beam-photon representation (e.g., a beam-query using photon-beams). 2.2 Ray Mapping 1.2 Contributions In order to develop a complete algorithm utilizing photon beams, we introduce a novel density estimation framework based around line segments. This allows us to extend standard volumetric photon mapping with a new data representation. We in fact go a step further and develop a generalized theory of radiance estimation in participating media which allows the use of points or beams for either the data or query representation, or both. Our main contribution is a theory that subsumes existing radiance estimates [Jensen and Christensen 1998; Jarosz et al. 2008; Schj th 2009] and also expands the collection to nine estimates by including the new photon beam data representation and query operation. To validate our new theory we develop a number of prototype volumetric rendering techniques utilizing these novel radiance Previous researchers have proposed the use of beams or rays to solve surface illumination problems. Lastra et al. [2002] stored photon trajectories with each photon and used them to locate photons that would have intersected the tangent plane of the query point in order to reduce boundary bias in the surface radiance estimate. Havran et al. [2005] developed a specialized data structure called the œray map and formulated a number of other metrics to couple a query location on a surface with photon paths. Herzog et al. [2007] reformulated ray maps by splatting photon energy to all surface measurement points along a photon ™s trajectory. Zinke and Weber [2006], on the other hand, discretely sampled the ray map back into photon points to improve query performance. None of these previous approaches, however, considered participating media. Our concept of photon beams is very similar in spirit to that of the ray map. However, we apply this concept to simulating lighting in participating media, which is more challenging since radiance does not remain constant along lines through a medium. Moreover, the bene t of using beams as the data representation is much greater for participating media than for surfaces since, in a volume, beams not only reduce boundary bias but, as we will show, significantly reduce variance by implicitly increasing photon density. 2.3 Beam Tracing Our use of photon beams is also related to the concept of beam tracing [Heckbert and Hanrahan 1984], where in place of A Comprehensive Theory of Volumetric Radiance Estimation in nitesimal rays, polygonal geometry is extruded to form thick œbeams which are re ‚ected and refracted in the scene. The concept of beam tracing was later applied in reverse to light paths to, for example, visualize caustics at the bottom of a swimming pool [Watt 1990]; however, neither of these techniques considered participating media. Nishita et al. [1987] used illumination volumes formed by spotlights to simulate single scattering and subsequently extended this algorithm to visualize underwater shafts of light [Nishita and Nakamae 1994]. Unfortunately, the resulting light beams have complicated boundaries and intensity pro les which are approximated by sampling and interpolation. These volumetric light beam tracing methods can run interactively on the GPU in simple scenes [Iwasaki et al. 2001; Ernst et al. 2005]. Kr ger et al. [2006] suggest a related u GPU technique where image-space photons create underwater lines of light which are then blurred in image-space; however, they do not strive for physical correctness and cannot handle absorption. The use of light beams to represent shafts of light within participating media is very similar to our concept of photon beams. In fact, one of the new estimates we introduce can be seen as a generalization of light beam tracing. Photon beams have a number of additional practical benefits. Firstly, light beams are tied to scene geometry and hence not well suited for scenes with high geometric complexity. Photon beams, on the other hand, are independent of the geometry, and can handle scenes with highly tessellated or even procedural geometry. Secondly, it is dif cult to handle higher-order scattering effects with light beams (e.g., multiple specular refractions) since subsequent bounces require nontrivial geometric light beam clipping. Photon beams are based on standard ray tracing and naturally handle multiple specular interfaces (for volumetric caustics) and even multiple scattering effects, which are not considered at all by light beam methods. Lastly, light beam tracing, unlike the approaches developed in this article, cannot easily handle area light sources. ¢ Fig. 2. Radiance reaching the eye L(xc c ) is the sum of surface radiance L(xs c ) and accumulated in-scattered radiance Li (xtc c ) along a ray. volume, so numerical integration using ray marching is still needed to compute the contribution along a camera ray. This integration can be expensive and, if not enough samples are taken, clamping is used to avoid noise which introduces unbounded bias due to loss of energy. We develop a theory to directly couple photon beams with entire camera rays, thereby eliminating the need to perform ray marching. 

3. BACKGROUND 

We will derive a novel theory of density estimation in participating media combining points and beams. In this section we describe the technical details of light transport in participating media and establish a consistent notation used throughout our exposition. Since we base our derivations on density estimation used in volumetric photon mapping, we also review the details of this algorithm. 3.1 Light Transport in Participating Media In a vacuum, photons travel unobstructed until they interact with a surface. In participating media, photons interact with the surrounding medium. At any point, a photon traveling through a medium may be scattered or absorbed, altering its path, and reducing the contribution in the original direction. This process is described by the Radiative Transfer Equation (RTE) [Chandrasekar 1960]. In its integral form, the RTE recursively de nes radiance reaching the eye xc from direction c as a sum of re ‚ected radiance from the nearest visible surface and accumulated in-scattered radiance from the medium between the surface and the camera (see Figure 2): L(xc c ) = Tr (xc ” xs )L(xs ’ c ) s 2.4 Exact/Direct Techniques Several techniques solve for the exact contribution of speci c light paths, without an intermediate representation. Mitchell and Hanrahan [1992] solve for the exact re ‚ection points off of curved surfaces and the corresponding lighting contribution, and discuss, but do not demonstrate, performing the same computation for refraction. This technique could be used to directly simulate dif cult light paths within participating media, such as re ‚ective caustics, but only with a single bounce and geometry limited to implicit surfaces. Recently, Walter et al. [2009] developed a similar technique for computing refraction paths within triangle meshes. By using triangles, the technique is similar to light beam tracing, and can likewise produce light shafts within complex refractive volumetric boundaries. However, the solution is exact so it does not suffer from blurring or approximation artifacts present in light beam tracing or photon mapping methods. In contrast, volumetric photon mapping could require extremely high photon counts to reconstruct similarly sharp lighting features. Our photon beam estimates implicitly increase the data density compared to photon particles, providing a similar bene t. The accuracy of Walter ™s technique, however, comes at a price. Firstly, it requires an expensive numerical solver for robustness, and performance is once again tied to the geometric complexity of the boundary mesh, making it impractical for complex scenes. Also, only a restricted class of light paths with a single refractive boundary are considered, whereas our methods can be used to simulate any number of bounces. Finally, Walter ™s technique solves for light contribution only at individual points in the + Tr (xc ” xtc ) s (xtc )Li (xtc ’ c ) dtc , (1) where Tr is the beam transmittance, s is the distance through the medium to the nearest surface at xs = xc ’ s c , and xtc = xc ’ tc c with tc ˆ (0, s). We summarize our notation in Table I. In-scattered radiance, Li , recursively depends on radiance arriving at xtc from all directions tc over the sphere of directions 4 : Li (xtc ’ c ) = f ( tc )L(xtc tc ) d tc , 4 (2) where f is the normalized phase function, and tc is the angle between the incident and outgoing directions at xtc : cos tc = c tc . The surface radiance, L(xs ’ c ), governed by the rendering equation [Kajiya 1986], serves as the boundary condition for the RTE. In heterogeneous media, the scattering properties may vary throughout the medium. In this case, we denote the scattering and absorption coef cients of the medium as s (x) and a (x), and the extinction coef cient is t (x) = s (x) + a (x). ¢ W. Jarosz et al. Table I. De nitions of Quantities Used throughout This Article Symbol x, Description Position, direction Sphere of directions 4 t Distance along a ray or beam Quantity associated with a camera ray, (e.g. xc , c , tc ) c Quantity associated with photon particle p, (e.g. xp , p , p ) p Quantity associated with photon beam b, (e.g., xb , b , tb ) b s , a , t Scattering, absorption, and extinction coef cients b Angle between photon beam and camera ray f ( ) Normalized phase function R, Rb Abstract query/blurring region, aligned with photon beam b Tr Beam transmittance: e ’ t t Flux (power) or a photon particle or beam L(x ) Incident radiance arriving at x from direction Li (x ’ ) Excitant in-scattered radiance leaving x in direction Lb (x , s) œBeam radiance : incident integrated in-scattered radiance arriving at x from media from direction . A canonical random number between 0 and 1 3.3 Notation This article deals with a wide variety of quantities, expressing all of which with absolute precision and generality would make equations unmanageably verbose. For conciseness, we will use the homogeneous RTE for most of the remainder of this article. When not immediately obvious, we discuss algorithmic changes needed to handle heterogeneous media. However, to make the meaning of terms more obvious in context, we will typically denote quantities relating to camera rays with a subscript c (e.g., xc , c , tc ), relating to photon particles with a subscript p (e.g., xp , p , p ), and relating to photon beams with a subscript of b (e.g., xb , b , tb ). We use superscripts sparingly to denote other dependencies. We also make a notational distinction between incident/incoming and excitant/outgoing radiance using arrow notation (see Table I). Our illustrations use green for quantities relating to the data (photon points and photon beams), red for quantities relating to the query (query point or camera ray), and blue to visualize query regions or blurring kernels. 3.4 The beam transmittance, Tr , gives the fraction of radiance that can be transported between two points along a ray, and is de ned as t (x+t ) dt . (3) Tr (x ” x ) = e ’ 0 In homogeneous media, t , s , and a do not depend on position, and a number of mathematical simpli cations can be made. Speci cally, the integral in Eq. (3) can be replaced by a simple product x ’x Overview Tr (x ” x ) = e ’ t x ’x , which allows us to simplify the RTE in Eq. (1) to L(xc c ) = e ’ t s L(xs ’ c ) + s 0 s (4) e ’ t tc Li (xtc ’ c ) dtc . (5) 3.2 Volumetric Photon Mapping Jensen and Christensen [1998] solve the RTE using a combination of photon tracing, ray marching, and density estimation. In a preprocess, packets of energy, or œphotons, are shot from light sources, scattered at surfaces and within the medium, and their interactions are stored in a global data structure. This photon tracing stage is typically implemented using a Markov random walk process, though a number of other sampling strategies are possible. During rendering, ray marching is used to numerically integrate radiance seen directly by the observer. For a homogeneous medium, this involves approximating Eq. (5) as S ’1 L(xc c ) ˆ e ’ t s L(xs ’ c ) + s tc =0 e ’ t tc Li (xtc ’ c ) tc , (6) In Sections 4 through 7, we explore volumetric radiance estimation with photon mapping in more detail. In particular, we derive a comprehensive theory of volumetric radiance estimation which leads to significant new insights and encompasses all previously published volumetric radiance estimates as special cases. We rst examine radiance estimation using photon points in Section 4 and then show how to generalize this concept to volumetric photon beams. Section 5 explains the photon beams concept and sets the mathematical foundation for radiance estimation using this representation. In Section 6 we derive two ways to estimate in-scattered radiance in a volume using photon beams, and in Section 7 we derive four ways to estimate the accumulated inscattered radiance along a camera ray directly using photon beams. In total, these generalizations result in nine distinct radiance estimates. We categorize these estimates based on the type of query (point or beam), the photon data representation (point or beam), and the dimensionality of the blur coupling these two terms (3D, 2D, or 1D). We demonstrate each of these estimators in Figure 3 and aggregate all the estimator formulae derived throughout this article in Table II. In Section 8 we provide practical details needed to implement several of the novel radiance estimates, and in Section 9 show and discuss rendered results using these methods. In Section 10 we discuss how our method ts into the larger scope of volumetric rendering, and how our theory sheds light on connections between other, seemingly disparate volume rendering approaches. Limitations and areas of future work are discussed in Section 11. 4. where tc is the length of segments along the ray and x0 , . . . , xs ’1 are the segment sample points (x0 and xs ’1 are the rst and last points within the medium, and xs is a surface point past the medium). The most expensive part to compute in Eq. (6) is the in-scattered radiance Li , because it involves accounting for all light arriving at each point xt along the ray from any other point in the scene. Instead of computing these values independently for each location, photon mapping gains ef ciency by reusing the computation performed during the photon tracing stage. The in-scattered radiance is approximated using density estimation by gathering photons within a small spherical neighborhood around each sample location. RADIANCE ESTIMATION USING PHOTON POINTS We detail all the possible ways to estimate radiance with photon points. Standard volumetric radiance estimation is reviewed in Section 4.1, we then present novel derivations for two different radiance estimates computed along the length of the eye ray. 4.1 Point Query Point Data, 3D Blur The original volumetric radiance estimate computes the in-scattered radiance at a query point by searching for photon points stored within the medium. In this estimate, the query is a point, x, and the A Comprehensive Theory of Volumetric Radiance Estimation ¢ Table II. The Nine Radiance Estimates Described in This Article, Categorized by the Query Type, the Data Representation, and the Dimensionality of the Blur Query Data (Blur) Radiance Estimate 1 Point Point (3D) f ( p ) s R (r 3 ) p ˆR Equation Number p + tp,c (7) e ’ t tc dtc (10) (12) (21) (25) dtc (27) (29) (33) (38) Beam Point (3D) Beam Point (2D) Point Beam (3D) Point Beam (2D) Beam Beam (3D) Beam Beam (2D)1 Beam Beam (2D)2 Beam Beam (1D) 1 R (r 3 ) 1 R (r 2 ) 1 R (r 3 ) 1 R (r 2 ) s R (r 3 ) s R (r 2 ) s R (r 2 ) s R (r) f ( p ) p ˆR p ’ tp,c f ( p ) p ˆR p e ’ t tp,c + tb f ( b ) b ˆR b ’ tb e ’ t tb dtb f ( b ) b ˆRb be ’ t tx,b f ( b ) b ˆR b f ( b ) b ˆRb e ’ t tc e ’ t tb dtb ’ ’ tc tb (tc ) + tc e ’ t tc e ’ t tb dtc b ’ tc + tb ’ tb c ’ t tb + + tc tb (tc ) f ( b ) b ˆR b b e ’ t tb e ’ t tc dtb b ’ t tc f ( b ) b ˆRb e e sin b A rendering resulting from each of these estimates is shown in Figure 3. 4.2 Beam Query Point Data, 3D Blur It is also possible to directly compute the accumulated in-scattered radiance along a camera ray, which we call œbeam radiance s Lb (xc c , s) = s e ’ t tc Li (xtc ’ c ) dtc . (8) Fig. 3. Single-scattering in a Cornell box rendered with each of the nine estimates (see Table II). We only visualize media scattering and intentionally use a small number of photons and a constant kernel to highlight each method ™s artifacts and blurring behaviors. 100k photon points (top row) and 5k photon beams (bottom two rows) are used to represent the illumination. The standard volumetric radiance estimate can be thought of as the convolution of the radiance at a point xtc with a spherical 3D volumetric kernel. Substituting Eq. (7) into Eq. (8) yields s Lb (xc c , s) ˆ s e ’ t tc s 0 1 f ( p ) s R (r 3 ) p ˆR(t ) c p dtc , (9) ˆ 1 R (r 3 ) e ’ t tc f ( p ) p ˆR(tc ) p dtc , data are points, xp . The resulting estimate blurs the values stored at the points xp in three dimensions [Jensen and Christensen 1998]: Li (x ’ ) = f ( )L(x )d ˆ 4 1 s R (r 3 ) f ( p ) p ˆR p, (7) where the summation loops over all photons p found within a 3D query region R. This sum is divided by the measure of the query region. For the typical spherical query the measure is simply the volume of a sphere R (r 3 ) = 4 r 3 , but other 3D queries are 3 possible, as well as weightings by nonconstant kernels. We illustrate this radiance estimate in Figure 4(a). Our notation may seem to suggest a restriction to spatially constant (i.e., nonadaptive) query regions, by not explicitly specifying a dependence on x or xp . This is only for simplicity and brevity of notation and is not a limitation of the methods described. We discuss the application of spatially adaptive blurring kernels in Section 8.1. where R(tc ) indicates that the 3D query region moves with tc along the beam integral domain. Unfortunately, Eq. (9) is not immediately useful as it involves convolving the query region with the line integral. We can instead look at the density estimation problem from the dual perspective where each photon is the center of a volumetric 3D kernel. This interpretation allows us to swap the order of the integral and summation, yielding the following radiance estimate 1 Lb (xc c , s) ˆ R (r 3 ) + tp,c f ( p ) p ˆR p ’ tp,c e ’ t tc dtc , (10) where the summation loops over all photons whose volumetric (typically spherical) kernels overlap the camera ray, tc is the distance ’ from xc along the camera ray, and the integration bounds tp,c and + tp,c are determined by the intersection of the camera ray with the kernel for photon p. We illustrate these quantities in Figure 4(b). ¢ W. Jarosz et al. (a) Point Point (3D) (b) Beam Point (3D) (c) Beam Point (2D) Fig. 4. Illustrations of the three possible radiance estimates using photon points. Radiance estimate (a) queries at a point and blurs in 3D, corresponding to the standard estimate introduced by Jensen and Christensen [1998]. Estimator (c) queries along a camera beam, only blurs in 2D, and corresponds to the beam radiance estimate introduced by Jarosz et al. [2008]. An estimator (b) which blurs in 3D along a camera beam is also possible. In a homogeneous medium, the integral of the transmittance term can be computed analytically as + tp,c ’ tp,c e ’ t tc dtc = e ’ t tp,c ’ e ’ t tp,c . t ’ + (11) 4.2.1 Discussion. An important distinction between the radiance estimates in Eq. (10) and (7) is that they estimate different radiometric quantities: Li (x ’ ) is an excitant quantity while Lb (xc c , s) is an incident quantity. To compute the incident radiance reaching the eye using the standard approach, numerical integration would need to be used by inserting Eq. (7) into the ray marching process of Eq. (6) (even for homogeneous media). In contrast, Eq. (10) computes the camera ray integral directly and, in homogeneous media, does not require ray marching. In effect, multiple ray marching evaluations of Eq. (7) can be replaced with a single evaluation of Eq. (10). Furthermore, as the number of ray marching samples increases to in nity, these two approaches will provide identical results. Note that even though these estimates compute different quantities, they can use the same photon map. which is unfortunately hidden in the de nition of the blurring kernel in the original expressions presented by Jarosz et al. [2008]. Jarosz et al. [2008] derived Eq. (12) using a reformulation of photon mapping in terms of the measurement equation [Veach 1997]. For completeness, we will show that it is possible to derive the exact same estimate without this reformulation by starting from Eq. (10). This derivation also allows us to examine the relationship between the 3D and 2D beam radiance estimates more precisely. 4.3.1 Derivation. Eq. (10) blurs photons with an abstract 3D kernel. The shape of this kernel does not in ‚uence the algorithm ™s correctness, only the behavior of bias and variance. In fact, this fact has been exploited previously by aligning kernels with structures in the lighting to minimize bias [Schj th et al. 2006, 2007, 2008]. Consequently, we could choose a 3D region other than a sphere to perform our blur (e.g., a cylinder). This only requires changing R 2 to express, for instance, the volume of a cylinder, R (r, h) = r 2 h. We start by expressing Eq. (10) using a cylindrical kernel Lb (xc c , s) ˆ 1 r 2h + tp,c f ( p ) p ˆR p 4.3 Beam Query Point Data, 2D Blur ’ tp,c e ’ t tc dtc . (13) Recently, Jarosz et al. [2008] introduced the so-called œbeam radiance estimate, where the contributions from all photons along a camera ray are queried at once, similarly to Eq. (10). Expressed in our notation, this beam radiance estimate can be written as1 Lb (xc c , s) ˆ 1 R (r 2 ) f ( p ) p ˆR p e ’ t tp,c , (12) where tp,c = (xc ’ xp ) c is the projected distance along the ray from the origin xc to photon p. The sum is over all photons within a region R and, to enforce integration bounds along the beam, only considers photons with 0 ¤ tp,c ¤ s. This blurs each photon into a œphoton disc perpendicular to the query ray, as in Figure 4(c). As with the previous estimate, this beam radiance estimate computes the 1D beam integral directly; however, it only blurs the contributions of each photon in 2D, perpendicular to the ray. Therefore we divide by the 2D measure of the region to compute the density. For a cylinder with a circular cross-section, R (r 2 ) = r 2 . This is an important distinction which may initially seem unintuitive, and 1 Jarosz If we also align cylinders with the camera ray, the extent of the + ’ integration bounds always equals the cylinder height: tp,c ’tp,c = h. h ’ + With this alignment, we express tp,c = tp,c ’ 2 , and tp,c = tp,c + h . 2 Integrating over a distance h and dividing by h (from the measure of the cylinder), we effectively compute the average transmittance through the cylinder. In the limit, reducing the cylinder ™s height to integrate smaller regions, this average becomes a simple evaluation › ž + tp,c 1 ’ t tc f ( p ) p e dtc Lb (xc c , s) ˆ lim 2 ’ h ’0 r h p ˆR tp,c = 1 r2 f ( p ) p ˆR p e ’ t tp,c . (14) Note that if we use a circular cross-section with R (r 2 ) = r 2 , this is identical to the beam radiance estimate in Eq. (12). 4.3.2 Discussion. We will discuss the relationship between the 2D and 3D beam radiance estimates. In Eq. (10), transmittance is integrated across the 3D kernel ™s depth, whereas in Eq. (12), transmittance is simply evaluated since the 2D kernel has no depth. However, in the 3D version we divide by an extra dimension. Eq. (14) ™s derivation solidi es the connection: combining the extra division and integral along the ray effectively averages the transmittance through the kernel. In the limit, we obtain a 2D blur, et al. [2008] use a different de nition of photon contribution and so their equations contain an additional s factor. Here we use notation to remain consistent with other sources [Jensen and Christensen 1998]. A Comprehensive Theory of Volumetric Radiance Estimation and this average becomes an evaluation of transmittance at the intersection of the kernel and camera ray. Strictly speaking, Eq. (10) and Eq. (12) produce very similar, but not identical results. The difference lies in the averaging of the transmittance term across depth in the 3D kernel. Since transmittance is nonlinear, averaging is not equivalent to evaluation at the 3D kernel ™s center. It is also instructive to discuss the connection between these beam radiance estimates and the photon splatting approach developed by Boudet et al. [2005] and Schj th [2009]. The splatting approaches consider a 3D region around each photon, and integrate each photon ™s contribution onto the image plane as a splat. This integration, however, only considers the value of the kernel through the 3D region, and not the transmittance which is evaluated at the center of the kernel. The resulting radiance estimate divides by the volume of the region, but multiplies by the 1D integral of the kernel along the ray. These two operations combined produce an effective 2D kernel surrounding each photon. Hence, the splatting estimates are mathematically equivalent to the 2D beam radiance estimate in Eq. (12). In homogeneous media both these estimates can be evaluated analytically, so the bene t of one over the other is not immediately obvious. In heterogeneous media, however, using Eq. (12) is more practical, since we only need to evaluate the transmittance (using ray marching) instead of averaging the transmittance through each photon. Eq. (12) ™s beam radiance estimate is computationally much more ef cient than the point-wise volumetric radiance estimate in Eq. (7) with ray marching (see Jarosz et al. [2008]). In heterogeneous media the beam radiance estimate requires ray marching, but only to compute transmittance to each photon. This allows us to nd all photons around the beam in one query, and take a weighted sum of their powers with Eq. (12). Eq. (7) is wasteful, requiring many queries per camera ray, which may result in the same photons being found multiple times. ¢ to the difference between synthesizing an image using random walk volumetric path tracing and ray marching from the camera. Ray marching numerically solves 1D integrals using a Riemann sum. A step-size t is chosen and the ray is sampled at discrete locations. We assume a uniform step size for simplicity of the derivations, which implies the medium is assumed to have nite extent. To employ ray marching in photon tracing, the propagation distance is no longer chosen once for a single photon (as in a random walk), but instead the power of each photon passing through the medium is distributed among photons deposited at uniform intervals along the length of the ray through the medium. We call this discrete collection of photons along a single photon marching ray a œdiscrete photon beam. Mathematically, this is expressed as pb = s e ’ t xb ’xt b t, (15) where b is the power of the photon beam upon entering the medium (the emitted photon power if the light is within the medium), the exponential computes transmittance from the ray origin to the current photon xt , and pb is the power of each photon. This scheme stores a photon at each xt = xb + t b , with t = ( + [0, . . . , N ’ 1]) t. The total number of photons deposited along the discrete beam, N , depends on the step-size and extent of the beam through the medium. Each beam uses a different random offset to avoid aliasing, but this is inconsequential for our derivations. 5.2 Multiple Scattering Any photon entering the medium induces a discrete photon beam through the entire length of the medium. To simulate multiple scattering, photon marching can be combined with random walk sampling. This is accomplished by scattering a new discrete beam off of the current beam. For discrete beam b0 , a random propagation distance ts is rst chosen for the scattering event location. In homogeneous media we compute this analytically based on the mean-free path as ts = ’ log(1 ’ ) with pdf (ts ) = t e ’ t ts . For heterogeneous me t dia, the inversion method [Pharr and Humphreys 2004] or deltatracking [Coleman 1968; Raab et al. 2008] can be used. After the location is chosen, a new discrete beam is initiated with origin xb1 = xb0 + ts b0 , direction b1 (sampled from f ( )), and a starting power of b1 = st b0 . The scattering albedo, st , can be used for Russian-roulette [Arvo and Kirk 1990], in which case the starting power of an accepted beam simpli es to b1 = b0 (in grey media). Even though the scattered beam is initiated at a distance ts along the parent b0 , photon marching of b0 continues to the media ™s boundary (see Figure 1). This simpli es our remaining derivations and provides the added bene t of increased photon density throughout the medium. 

5. PHOTON BEAMS OVERVIEW 

We now generalize the concept of photon mapping to use photon beams, resulting in several novel radiance estimates. The most commonly described photon shooting method is based on random walk sampling where each emitted photon traces a Markov-chain of scattering events through the scene; however, other approaches are possible as well.2 Here we describe a different form of photon shooting which resembles ray marching from the lights, and call this process œphoton marching. This approach can be much more effective in certain scenarios (e.g., light through a stained-glass window). Though this technique is not new (it was, for instance, used to generate volumetric water beams by Jensen and Christensen [1998]), it is not widely known and, to our knowledge, not described in the literature. More importantly, our de nition of photon beams is based on the concept of photon marching, so we describe this process here in more detail. However, we only use this process to derive a mathematical de nition of photon beams, and in practice photon beams are still created using a random walk process. 

6. IN-SCATTERED RADIANCE ESTIMATION USING PHOTON BEAMS 

5.1 Photon Marching Photon marching is identical to ray marching with rays originating at the lights. At each step, instead of sampling the in-scattered light, a photon is deposited. The distinction between photon tracing using a Markov random walk or a photon marching process is analogous 2 Any In this section we generalize the theory of radiance estimation to compute in-scattered radiance at a point using photon beams. 

6.1 Point Query Beam Data, 3D Blur We derive our rst in-scattered radiance estimate at a point in the medium due to photon beams by combining the photon marching concept with the volumetric radiance estimate in Eq. (7). Each discrete photon beam contains photons pb deposited with photon strategy is valid as long as the joint probability density of the photon powers, positions, and directions satis es certain conditions [Veach 1997]. ¢ W. Jarosz et al. â‡’ â‡’ (a) Point Beam (3D) (b) limit of discrete photon marching (c) Point Beam (2D) Fig. 5. In-scattered radiance estimation with photon beams. Radiance estimator (a) computes in-scattered radiance at a point using photon beams with a 3D blur. Taking the limit of discrete photon marching (b) while shrinking the cylindrical kernel ™s height, we can obtain a similar estimator (c) which only blurs in 2D. marching. Rewriting Eq. (7) in terms of these photons yields Li (x ’ ) ˆ 1 s R (r 3 ) f ( pb ) b ˆR pb ˆR pb , (16) where tb is the parametric distance along beam b. The integral bounds tb ˆ R represent the range along beam b that overlaps with ’ + region R. De ning overlap endpoints, tb and tb , Li is expressed. Li (x ’ ) ˆ 1 R (r 3 ) + tb where the outer sum iterates over all discrete beams b overlapping region R, and the inner sum nds all photons within the search region belonging to beam b. Note that this is exactly equivalent to Eq. (7), we have just split up the single summation into two. As the step size of a discrete photon beam goes to zero, we call the limit a œcontinuous photon beam. Conceptually, a photon resides at each point along a continuous beam. Mathematically, we substitute the photon power expression (Eq. (15)) into Eq. (16) and move the phase function and power out of the inner sum, since discrete photons on a beam share incident directions Li (x ’ ) ˆ = 1 s R (r 3 ) 1 R (r 3 ) f ( pb ) s b ˆR pb ˆR b f ( b ) b ˆR b ’ tb e ’ t tb dtb . (21) In homogeneous media, the integral of transmittance can be solved analytically (Eq. (11)). We illustrate this estimate in Figure 5(a). 6.1.1 Discussion. Eq. (21) is a true volumetric estimate, averaging over a 3D region and dividing by the volume measure R (r 3 ). This can be thought of as the convolution of the continuous photon beam function with a 3D blurring kernel. It is interesting to note that the point query of a 3D blurred photon beam in Eq. (21) and the beam query of 3D blurred photon point in Eq. (10) are nearly identical. They differ only in the direction transmittance is computed. The estimate in this section evaluates transmittance from the query point to the start of the beam and integrates it around the query point; in contrast, the estimate in Section 4.2 evaluates transmittance from the photon point towards the eye and integrates it around the photon point. In a sense, these estimates are radiometric œcomplements of each other and, due to the bidirectionality of light transport, end up being nearly identical mathematically. This concept of complementary radiance estimates will appear several more times in our derivations. More precisely, it occurs anytime we swap query and data representations but maintain the blur dimensionality. e ’ t tp,b t (17) f ( b ) b ˆR b pb ˆR e ’ t tp,b t, where b and b are the direction and power of discrete beam b, b is the angle between the eye ray and the beam (cos b = c b ), and tp,b is the distance from the start of the beam xb to photon pb . To arrive at continuous beams, we take the limit as t goes to zero: Li (x ’ ) ˆ = 1 lim f ( b ) R (r 3 ) t ’0 b ˆR 1 R (r 3 ) f ( b ) b ˆR b b pb ˆR e ’ t tp,b e ’ t tp,b pb ˆR t t. (18) lim 6.2 Point Query Beam Data, 2D Blur t ’0 In the limit, a Riemann sum computes an integral, lim x ’0 f (x) x = x ˆR x ˆR f (x) dx, (19) hence, as we increase the number of photons along a beam to in nity, the inner sum in Eq. (18) becomes a continuous 1D integral along each beam Li (x ’ ) ˆ 1 R (r 3 ) f ( b ) b ˆR b tb ˆR e ’ t tb dtb , (20) Photon mapping can be viewed as computing random samples of the energy distribution in the scene [Veach 1997; Jarosz et al. 2008]. The value at each photon point is exact, but a blur is required to utilize these samples for estimating radiance at arbitrary points in the scene. With the standard volumetric radiance estimate (Eq. (7)) we blur the photon energy in three dimensions to obtain a valid estimate at all points within the volume. Eq. (21) also blurs in 3D. Continuous photon beams, however, express the exact value of the lighting at all points along the photon beam, not just at a single point. Hence, blurring along the length of the beam only serves to introduce bias. We can exploit this observation to obtain another radiance estimate for photon beams which only blurs in 2D, perpendicular to the beam. A Comprehensive Theory of Volumetric Radiance Estimation We utilize the dual interpretation of density estimation, where a blurring kernel is placed at each data œpoint : in this case, the standard volumetric radiance estimate places a spherical kernel at each photon point and the in-scattered radiance is accumulated from the energy of all photon-spheres that overlap the query point. As mentioned earlier, the blurring kernel shape only in ‚uences the bias and variance behavior, not the correctness of the algorithm. We exploit this fact, and again employ a cylindrical blurring region (this time aligned with the photon beam) to derive the next radiance estimate. The cylindrical blur can be expressed using the discrete photon beams with only a slight modi cation to Eq. (17). We have 1 f ( b ) b e ’ t tp,b t, (22) Li (x ’ ) ˆ r 2 h b ˆR p ˆR b b b ¢ where the rst sum loops over beams that pass near the query location x, and the second nds all photon-marching photons for that discrete beam that fall within the oriented cylindrical region Rb . The orientation of the cylindrical blur depends on the beam, denoted Rb . To obtain continuous photon beams, we could repeat a similar procedure as in the previous section by taking the limit as t goes to 0 and arrive at an expression analogous to Eq. (21). However, we can reduce bias by simultaneously reducing the blur along the length of the beam to zero, as illustrated in Figure 5(b). To do so, we set the cylinder height to the photon-marching step-size, h = t 1 Li (x ’ ) ˆ lim f ( b ) b e ’ t tp,b t. (23) t ’0 r 2 t b ˆR p ˆR b b b in fact be mathematically identical. The 2D estimate presented here has a number of advantages. Firstly, it is simpler to evaluate since it does not involve integrating along the length of beams. Secondly, since no blurring is performed along the beam, this radiance estimate introduces less bias for the same radius. It can be thought of as the limit case of an anisotropic density estimation. Comparing Eq. (25) to the beam radiance estimate (Eq. (12)), we see that even though these compute very different quantities (in-scattered radiance at a point versus accumulated in-scattered radiance along an eye ray), they have a very similar structure and are in fact complementary. Both estimates divide by the cross-sectional area of either the camera beam or the photon beam, and both weight the photon contributions by a transmittance term (though computed in different directions). This elegant similarity is not coincidental, arising again due to the bidirectionality of light transport. Eq. (25) is also similar to the surface radiance estimates developed by Herzog et al. [2007] and can be seen as a generalization of their photon ray splatting estimate to participating media. 

7. BEAM RADIANCE ESTIMATION USING PHOTON BEAMS 

We now consider direct computation of the beam radiance integral using photon beams; hence, all remaining estimates apply beam queries to photon beams. 7.1 Beam Query Beam Data, 3D Blur To directly estimate the beam radiance integral with 3D blurred photon beams, we substitute Eq. (21) for the in-scattered radiance in the beam radiance integral along the camera ray (Eq. (8)): s To simplify this expression, we note that none of the kernels for a single beam overlaps because the photon spacing and size of the kernels are equal (see Figure 5(b)). This means that for any query point x, at most one photon (the one closest to x) from each discrete beam contributes to the estimate. This allows us to eliminate the inner summation. After solving for the limit behavior, we have 1 f ( b ) b e ’ t tx,b , (24) Li (x ’ ) ˆ r 2 b ˆR b Lb (xc c , s) = s e ’ t tc Li (xtc ’ c ) dtc s 0 ˆ s R (r 3 ) e ’ t tc b ˆR + tb (tc ) f ( b ) b ’ tb (tc ) e ’ t tb dtb dtc . (26) where tx,b = (x ’ xb ) b is the scalar projection of x onto beam b. Even though we start with a volumetric blur, the limit process collapses the cylinder to a disc resulting in a 2D blur normalized by the cylindrical cross-section area, r 2 , as illustrated in Figure 5(c). We used a circular cross-section for convenience of derivation, but an estimate using an arbitrary cross-section can be expressed as Li (x ’ ) ˆ 1 R (r 2 ) f ( b ) b ˆRb b Note that the bounds of the inner integral depend on the location along the camera ray tc . After moving the integral inside the summation, and de ning per-beam integration bounds, we arrive at Lb (xc c , s) ˆ s R (r 3 ) + + tc tb (tc ) f ( b ) b ˆR b e ’ t tx,b . ’ ’ tc tb (tc ) e ’ t (tc +tb ) dtb dtc , (27) (25) ’ tc + tc 6.2.1 Discussion. It is informative to compare this radiance estimate to the other ones presented so far. When compared to Eq. (21), we observe that the main differences are that here we divide by the 2D area measure instead of the 3D volume measure, and we also only need to evaluate the transmittance term along the beam, instead of integrating it along an overlapping region. Though different mathematically, these expressions estimate the same radiometric quantity (the two differences effectively cancel each other out). Roughly speaking, if we integrate the transmittance and divide by an extra r term, this is approximately equal to simply evaluating the transmittance. The extra division can be thought of as computing the œaverage transmittance through the region. In the limit, this is equivalent to evaluating the transmittance and only dividing by the area measure. Note that if the function being integrated (transmittance) were linear, these two formulations would where and are the overlap of the camera ray with the extruded 3D kernel along the photon beam, and at each point in this range we ’ + integrate along the length of the photon beam from tb (tc ) to tb (tc ). 7.1.1 Discussion. Unfortunately, this radiance estimate is fairly complicated since it involves the integral of a 3D convolution. This computation may be possible to express analytically for homogeneous media if we were to write out the expressions for the integration bounds more explicitly. However, for heterogeneous media, this estimate is fairly impractical and included simply for completeness. Practically, the remaining estimates we derive are much more useful. 7.2 Beam Query Beam Data, 2D Blur As discussed earlier, reducing the blur dimensionality (and replacing integration with evaluation) increases radiance estimate ef ciency. ¢ W. Jarosz et al. (a) Beam Beam (2D)1 (b) Beam Beam (2D)2 (c) Beam Beam (1D) Fig. 6. Illustrations of beam radiance estimation using photon beams. We describe two possible alternatives for blurring beams in 2D. Estimator (a) blurs perpendicular to the photon beam, expanding each photon beam into a cylinder. Estimator (b) blurs perpendicular to the camera ray, resulting in a skewed cylinder about the photon beam. The nal estimator (c) simpli es these to a 1D blur by compressing the cylinder into a camera-aligned rectangle. We can derive a radiance estimate which directly computes the beam radiance along a camera ray, blurring each photon beam with a 2D kernel. Two possible routes to derive this type of estimate exist. We could take the 2D continuous photon beam estimate for in-scattered radiance at a point (Eq. (25)) and integrate along the eye ray by substituting into Eq. (8). Alternatively, we could start with the beam radiance estimate using photon points in Eq. (12), substitute the photon points with the photon-marching process, and take this discrete marching process to the limit to obtain continuous photon beams. We derive both approaches and discuss their similarities and differences. 7.2.1 Integral of Continuous Photon Beams. Integrating Eq. (25) along the eye ray by inserting into Eq. (8) yields s 2005; Pegoraro and Parker 2009]. For beams with a constant crosssection area, this can be simpli ed even further. To do this, we must express the projected distance along the photon beam tb in terms of the integration variable tc . The relationship between these two terms, illustrated by the triangle formed in Figure 6(a), is: ’ ’ ’ tb = tb ’ |cos b |(tc ’ tc ), where tb is the scalar projection of the ’ integration start point tc , expressed as a distance along the photon beam, and cos b = c b . This allows us to solve the integral analytically as + tc ’ tc e ’ t tc e ’ t tb dtc = = c + tc ’ tc e ’ t tc e ’ t (tb ’(tc ’tc ’ + ’tc )(|cos b | ’1) ’ ’ )|cos b |) dtc (30) e ’ t (tc e t (tc ’ ’1 Lb (xc c , s) = s e ’ t tc s Li (xtc ’ c ) dtc e ’ t tc b ˆRb ’ +tb ) t (|cos b | ’ 1) . s ˆ R (r 2 ) f ( b ) b e ’ t tb dtc . (28) This continuous 2D photon beam estimate blurs the energy of each photon beam along a 2D kernel perpendicular to the direction of the beam. The extent of this kernel along the beam forms the region Rb . Using the dual-interpretation of density estimation, we can swap the order of integration and summation, and de ne integration bounds per beam to arrive at Lb (xc c , s) ˆ s f ( b ) R (r 2 ) b ˆR b + tc 7.2.2 Limit of Discrete Photon Beams. Alternatively, discrete photon-marching beams may be inserted into Eq. (12). We have s Lb (xc c , s) ˆ f ( pb ) pb e ’ t tp,c , (31) R (r 2 ) b ˆR p ˆR b b ’ tc e ’ t tc e ’ t tb dtc . (29) where tp,c = (xc ’ xpb ) c is the scalar projection of the photon position onto the camera ray. Expanding the discrete photon beam power pb into Eq. (31) and rearranging terms, we get s f ( b ) b e ’ t tp,b e ’ t tp,c t Lb (xc c , s) ˆ R (r 2 ) b ˆR p ˆR b s = R (r 2 ) f ( b ) b ˆR b pb ˆR e ’ t tp,b e ’ t tp,c t. (32) The integration is along the length of the camera ray, and the bounds ’ + tc and tc are the intersection distances of the camera ray with each photon beam region. The integrand is a product of two transmittance terms: one along the length of the camera ray and the other along the length of the photon beam. For beam regions that vary along the length of the beam (such as cones), the region measure R (r 2 ) is a function of tb and needs to be moved inside the integral. We illustrate this estimate in Figure 6(a). In homogeneous media, the integral of the two transmittance terms can be computed analytically. For conical shaped beams, the integral becomes the well-known airlight integral [Sun et al. Note that, as in the previous derivation, this results in two transmittance terms: one which computes the transmittance from the photons towards the origin of the photon beam and one which computes the transmittance from the photons towards the origin of the camera ray. To obtain continuous beams, we take the limit of discrete beams s f ( b ) b lim e ’ t tp,b e ’ t tp,c t, Lb (xc c , s) ˆ t ’0 R (r 2 ) b ˆR p ˆR b A Comprehensive Theory of Volumetric Radiance Estimation which replaces the inner summation with an integral Lb (xc c , s) ˆ s f ( b ) R (r 2 ) b ˆR + tb ¢ Front view Side view Front view Side view b ’ tb e ’ t tb e ’ t tc dtb . (33) Here the integration is performed along the length of the beam b, not ’ + the camera ray. The integration bounds tb and tb are determined by the overlap between the camera ray and the extrusion of the 2D blur kernel along the continuous photon beam. The second transmittance term uses tc , the projected distance along the camera ray of the point ’ at tb . The relation between these distances is tc = tc ’ |cos b |(tb ’ ’ ’ ’ tb ), where tc is the scalar projection of the integration start point, tb , expressed as a distance along the eye ray, and cos b = c b (see Figure 6(b)). We can use this relationship to compute the integral analytically by following a similar procedure as in Eq. (30). 7.2.3 Discussion. The estimates in Eqs. (29) and (33) appear nearly identical and are indeed complements of each other (recall that complementary estimates have the same blur dimensionality, with swapped query and data representations). However, unlike previous complementary estimates, these actually compute the same radiometric quantity, making them even more similar. Though nearly identical, we highlight a number of conceptual differences. Both estimates weight photon powers with an integral of two transmittance terms: one towards the origin of the photon beam and another towards the origin of the camera ray. The main difference between these integrals is that, in Eq. (29), the integration is performed along the camera ray, whereas in Eq. (33) the integration is performed along the photon beam. This difference comes about from the fact that in Eq. (29) the energy of the photons is always blurred perpendicular to the photon beam (forming a cylinder around the beam, as in Figure 6(a)), whereas in Eq. (33) the energy of the photons is always blurred perpendicular to the camera ray (forming a sheared cylinder about the beam, as in Figure 6(b)). This results in slightly different bias between these estimates. This can be better understood by examining the behavior of both estimates in the same situation. When the camera ray and the photon beam are perpendicular, cos b = 0 and in Eq. (33) the transmittance towards the camera is computed exactly (not integrated) since + ’ tc ’ tc = 0. On the other hand, the situation is reversed for the radiance estimate in Eq. (29): when cos b = 0, the transmittance towards the origin of the photon beam can be pulled out of the integral in Eq. (30). The two estimates effectively make different bias trade-offs when computing the two transmittance terms: equal bias is introduced in both estimates if the same radius is used, but depending on the relative orientation of the camera ray and photon beam, this bias is distributed differently between the two transmittance terms. Note, however, despite this slight difference, both estimates converge to the same answer in the limit. We did not nd a noticeable difference between the results of these two estimates in our comparisons. Of all the radiance estimates presented in this article, the two presented in this section are the most related to the beam tracing volume caustics approach developed by Nishita and Nakamae [1994]. Fig. 7. To obtain the Beam Beam (1D) estimate in Figure 6(c), we consider the limit of nonoverlapping, rectangular kernels facing the camera ray. to compute the amount of overlap in the limit. In Section 6.2 we were faced with a similar situation: 3D kernels would overlap when replicated along a discrete photon beam. In that situation, we were able to construct the kernels to never overlap, and in the limit the integration was eliminated, resulting in a new radiance estimate. We apply this same principle to obtain a new beam beam estimate. We proceed similarly to Section 7.2.2, but design our 2D kernels to never overlap by construction. We again start with Eq. (12) and insert the discrete photon-marching beams as we did in Eq. (31); however, this time we will use rectangular kernels perpendicular to the camera ray and oriented with the photon beam s f ( pb ) pb e ’ t tp,c , (34) Lb (xc c , s) ˆ hw b ˆR p ˆR b b b where w is the width of the rectangular kernel perpendicular to the photon beam and h is the height of the kernel parallel to the photon beam. Expanding the discrete photon beam power pb yields s Lb (xc c , s) ˆ f ( b ) b e ’ t tp,b e ’ t tp,c t hw b ˆR p ˆR b b b s = f ( b ) hw b ˆR b b e ’ t tp,b e ’ t tp,c t. (35) pb ˆRb In order to prevent the photon kernels from overlapping during this expansion, the height h must be a function of the photonmarching step-size t. Since the kernels are always perpendicular to the camera, they will not overlap when h = t sin b . This is illustrated in Figure 7. Since only one kernel ever overlaps the ray, this eliminates the need for the inner summation, Lb (xc c , s) ˆ f ( b ) s tw b ˆR b b e ’ t tp,b e ’ t tp,c sin b t. (36) We again take the limit of discrete beams to obtain continuous beams s f ( b ) b e ’ t tp,b e ’ t tp,c t, (37) Lb (xc c , s) ˆ lim t ’0 tw b ˆR sin b b 7.3 Beam Query Beam Data, 1D Blur and, after expressing the kernel bandwidth w with the abstract notation used for the previous radiance estimates, we arrive at Lb (xc c , s) ˆ s f ( b ) b e ’ t tb e ’ t tc . R (r) b ˆR sin b b b c When we use beams for both the query and the photons, the blur dimensionality can be reduced even further. In this section we show how to obtain a 1D beam beam blur. To derive Eq. (33) we took the 2D beam radiance estimate for photon points to the limit using photon marching. In the resulting estimate, the 2D kernels around each photon-marching photon overlap when projected onto the camera ray, and the integral is necessary (38) c In this continuous 1D formulation, tb is the distance along the photon b beam to the point closest to the camera ray, and tc is the distance along the camera ray to the point closest to the photon beam. We illustrate this radiance estimate in Figure 6(c). ¢ W. Jarosz et al. 7.3.1 Discussion. As in Section 6.2, we started with a radiance estimate of dimensionality d and, due to the limit behavior of the anisotropic blurring kernel, obtained a new estimate of reduced dimension d ’ 1. In Section 6.2 we went from an estimate that blurs in 3D to one that blurs in 2D along the beam. In this nal beam beam radiance estimate (Eq. (38)), the power of the photon beams are blurred in only one dimension, along the direction perpendicular to both the camera ray and photon beam. It is useful to discuss the connection between the 2D beam beam radiance estimates in Eqs. (29) and (33) with the one just derived in Eq. (38). Eq. (29) can be thought of as constructing a volumetric cylinder around each continuous photon beam, and integrating the power of each cylinder along the eye ray, that is, individual photon beams would look like smoky volumetric cylinders when rendered on the screen. Eq. (38) on the other hand, does not involve any integration and only divides by a 1D length measure R (r). Conceptually, this can be thought of as replacing all the cylinders with ‚at rectangles which always face the camera (i.e., billboards). This conceptual interpretation is quite useful for implementation, which we discuss in more detail in Section 8. Another aspect which deserves some discussion is the 1/ sin b term. By replacing the cylinders with billboards, the integration is eliminated and replaced by a simple evaluation. However, the bounds of this integration (i.e., the distance the camera ray travels through each beam) depend on the relative orientation of the camera ray and the photon beams. If these are perpendicular, the extent of the integration will be roughly the width of the photon beam, but as they stray from perpendicularity, the extent of the camera ray through the photon beam cylinder will increase. The sin b factor takes this change into account at a differential level, similar to the cos foreshortening term in the surface re ‚ectance integral. Unfortunately, the sin b term is in the denominator, which may initially seem problematic due to the potential singularity at b = 0, when the photon beam and camera ray are parallel. Furthermore, the transmittance terms are unde ned when the beams are parallel, because in this con guration there is no single point closest between two parallel lines. However, we found that in practice this estimate does not suffer any such numerical problems. Fig. 8. Fixed-width beams (top) give suboptimal results since the amount of blur is not adapted to the local beam density. Adaptive-width beams based on photon differentials (bottom) successfully reduce the blurring in focused regions such as the caustic, while simultaneously eliminating artifacts (e.g., banded lines emanating from the caustic) due to insuf cient blur. 

8. IMPLEMENTATION 

To validate and analyse the nine radiance estimates, we implement each within a simple C++ ray tracer. We followed the details of Jensen [2001] and Jarosz et al. [2008] for ef ciently implementing the standard point point 3D and beam point 2D Beam Radiance Estimates (BRE). Only a minor modi cation to the BRE is required to obtain our beam point 3D estimate from Section 4.2. Photon beams can also be easily integrated into an existing volumetric photon mapping implementation. The photon shooting preprocess remains almost completely unchanged: beam segments are stored wherever standard volume photons are stored, with initial beam start points originating at a light source. The only difference occurs at the intersection of photon paths with solid objects: while standard volumetric photon tracing discards a photon if it intersects an object prior to reaching its propagation distance, photon beam tracing stores a beam with an endpoint at the object intersection. The photon beams map contains a superset of the information in a standard photon map, as illustrated by the photon paths in Figure 1. simplicity, the theory in the previous sections has assumed an abstract, xed blurring kernel. A xed-width blur can easily be implemented by interpreting each photon beam as a cylinder of a speci ed radius (Figure 8 top). However, choosing a single blurring width that works well for the whole domain is often impossible, a problem that has been well studied in density estimation literature [Silverman 1986]. A better technique is to adjust the kernel width based on the local photon beam density. Standard photon mapping uses the k-nearest neighbors to locally adjust the search radius, while Jarosz et al. [2008] used the variable kernel method with a pilot density estimate to assign radii to photon points. Unfortunately, reusing either of these techniques is challenging in our context since photon beams are higher dimensional and we would like the blur radius to potentially vary along the length of the photon beams according to the local density. We explored several approaches for adapting beam widths, including pdf-based methods [Herzog et al. 2007, Suykens and Willems 2000, 2001], and techniques based on pilot density estimates at beam vertices [Jarosz et al. 2008]. Unfortunately, these approaches did not produce satisfactory results. Instead, we found that photon differentials [Schj th et al. 2007] provide an effective and automatic solution for estimating the divergent spreading and convergent focusing of photon beams as they propagate within a scene. 8.1 Adaptive Beam Width In order to estimate the radiance from a photon beams map, we need to be able to compute the local density of beams. For Photon Differentials. Photon differentials are the analog of ray differentials [Igehy 1999] from the direction of the light. Ray differentials were originally developed to estimate the footprint of camera rays for improved texture ltering, but have also been applied in œreverse to estimate desired photon footprints or blur-radii for surface-based [Schj th et al. 2007; Fabianowski and Dingliana 2009] as well as volumetric photon mapping [Schj th 2009]. We use photon differentials to determine the size and shape of each photon beam. A photon differential consists of the photon beam position and direction, (x, ), as well as two auxiliary rays, (xu , u ) and (xv , v ), with offset origins and directions. Together, the central beam ray and its two differential rays form a truncated cone, or conical frustum, which de nes the size and shape of each photon beam for density estimation. At any point t along the beam, the elliptical crosssection of the frustum has semimajor and semiminor axis lengths equal to the Euclidean distance between the point on the central ray, A Comprehensive Theory of Volumetric Radiance Estimation ¢ Fig. 9. We use photon differentials to compute the conical blurring frustum around each photon beam. For point light emission (left), the differential ray directions determine the spread of the cone, whereas for area lights (middle), the positional differentials are also considered. Specular interactions propagate the differentials. Multiple scattering beam differentials (right) are always parallel to the parent beam and spaced at a distance equal to the parent ™s radius at the scattering location x. Fig. 10. Adaptive photon beam width for an area light source rendered using our photon differentials (left), compared to a roughly equal-time (middle) and roughly equal quality (right) renderings with photon points. r(t) = x + t , and the corresponding points on the differential rays, ru (t) = xu + t u and rv (t) = xv + t v . At re ‚ective and refractive surface interactions, we modify differential rays according to Igehy [1999]. For example, in Figure 8, photon beam differentials are modi ed to focus and spread the beams of light according to the refraction into, and out of the glass sphere. We found that this approach works very well, handling extreme spreading and focusing caused by caustic light paths. (since the width would be zero), while this singularity would be correctly avoided for area light sources due to the nite crosssection. Light Source Emission. Light source emission affects the initial position, direction, and width of each emitted beam ™s differential rays. Generally, we aim to set differentials so as to create a tight packing of beams over the spatial and angular extent of the light source. We adopt different emission schemes for different light sources. More speci cally, the position and orientation of differential rays is determined by the spatial and directional PDFs of photon emission, and the total number of emitted photons from a light. For singularity lights, such as point-, spot-, and cosine-lights, the differential ray origins are set to the location of the light source. We set the differential ray directions such that the entire angular extent of the source is covered by the number of emitted beams, while minimizing the amount of overlap between beams. More precisely, if we emit N photon beams, the solid angle spanned by a beam ™s footprint is set to 1/(pdf( ) N ), where pdf( ) is the probability density of emitting a photon with direction . For instance, for an isotropic point-light, each photon beam footprint would be allotted 4 /N steradians. Since each beam forms a cone, we can therefore solve for the necessary apex angle between the central ray and the photon differentials. This process is similar to the one used by Fabianowski and Dingliana [2009], but extended to work with general angular distribution. We illustrate this in Figure 9. Our implementation also supports area light sources. In this case, the surface area PDF must also be considered. As with directions, we allot a portion of the light ™s surface area to each of the emitted photon differentials. Conceptually, we discretize the light ™s surface into a jittered grid and assign central and differential ray origins based on the jittered grid centers and edges, respectively. Differential and central beam directions are set according to the PDF of the angular emission of the source (see Figure 9). Figure 10 shows a scene that uses photon differentials for area light source emission. With this photon differential scheme, the radiance estimate would correctly return in nity at the position of a singularity light source Multiple Scattering. Just as multiple scattering photons can be deposited during standard photon shooting, multiple scattering beams can also be deposited with a random-walk shooting process. We have investigated several differential modulation schemes for multiple scattering beams, including one inspired by the recent œabsorb-and-reemit analogy suggested by Fabianowski and Dingliana [2009] for diffuse inter-re ‚ections, as well as pdf-based approaches [Suykens and Willems 2000, 2001, Herzog et al. 2007, Schj th 2009]. Unfortunately, we did not nd these techniques to work well with photon beams since the extreme spreading of beams resulted in visibly biased results. Instead, for multiple scattering, beams are always cylinders with widths set according to the footprint of the parent beam at the multiple scattering event location (see Figure 9). We found that this automatic process produces reasonable results. Figure 13 compares photon beams to standard photon mapping in a scene with two bounces of multiple scattering. 8.2 Global Beam Width and Smoothing Kernels In addition to using photon differentials to automatically predict the spreading and focusing of photon beams, we provide the user with a width multiplication factor, , which can globally increase or decrease the computed beams widths. This serves the same purpose as the k parameter used in standard k-nearest neighbor density estimation, and allows the user to adjust the tradeoff between banding and blur in the rendered image. Furthermore, since we allow the user to choose a smooth weighting kernel, some amount of overlap ( > 1) is desirable for optimal results. We experimented with several blur widths and smoothing kernels (including constant, cone, 3rd- and 5th-order smoothstep, biweight, and Gaussian) and found the biweight kernel [Silverman 1986] (K(x) = 15 (1 ’ x 2 )2 for 16 x ˆ [0, 1]) with an = 3.0 strikes a good balance between kernel overlap, blurring and banding. We used this kernel in all our results. 8.3 Photon Beam Storage To compute the beam radiance estimate, we need an ef cient way to intersect a camera ray with the collection of photon beams in the scene. Inspired by work in ray-hair intersection [Nakamaru and ¢ W. Jarosz et al. Fig. 11. We compare the error convergence of photon beams vs. photon points on a log-log scale in a test scene containing a focused lighting effect similar to a volumetric caustic. Our method requires 10,000 times fewer photon beams than photon points to obtain the same RMS error. A crop of the reference solution (top) is shown, as well as the photon points (middle) and photon beams (bottom) reconstruction using 100 photon point and beams respectively. Ohno 2002; Apodaca and Gritz 1999], we have explored several acceleration schemes (including KD-Trees, BVHs and grids). After experimenting with these techniques we found that, for our problem domain, a BVH with some modi cations performs quite well. We store photon beams in a BVH constructed using the surface area heuristic [MacDonald and Booth 1990]. Unfortunately, since many beams may overlap the same region of space (and in fact may share a common vertex such as all beams emanating from a point light source) a naive application of a BVH is inef cient. Ä± To improve partitioning by the BVH, we split beams into subbeams. The splitting length is set automatically by our construction algorithm to produce sub-beams with approximately unit aspect ratio. At runtime, we are able to quickly obtain all beams intersected with an eye ray, allowing rapid radiance estimation. 

9. RESULTS 

We have implemented all nine radiance estimates to validate the theory, and demonstrate the effectiveness of photon beams. All render times for these estimates were measured on a MacBook Pro with a 3.06 GHz Intel Core 2 Duo processor and 4 GB of RAM. Note that our current implementation uses only one core; however, since radiance estimation is a read-only operation, all estimates could easily be parallelized in an optimized implementation. The Cornell box, bumpy sphere, and sphere caustic images are 512 pixels across their larger dimension while all other images are 1024 pixels. All results were rendered with up to 16 samples per pixel. Figure 3 shows a Cornell box rendered with each of the estimators. Note that even though the query domains and data representations are vastly different among the estimators, the resulting images all faithfully approximate the same solution. Here our intent is to verify the correctness of all the derived estimators, and not on performance, so we omit render times. In our remaining results we demonstrate the benefits of photon beams over photon points. We compare to the previous state-ofthe-art Beam Point 2D estimate in terms of image quality and performance. We found that for this estimator the extra cost of computing photon radii using a pilot density estimate [Jarosz et al. 2008], as opposed to using photon differentials [Schj th 2009], is negligible and produces superior results. We therefore use this approach for all photon points results in our comparisons. For photon beams, we focus on the Beam Beam estimators since they avoid ray marching. From a theoretical perspective, all Beam Beam estimates provide a bene t over photon points in terms of image quality. However, as mentioned in Section 7.1, the Beam Beam 3D estimate is fairly expensive to implement since it requires a 3D convolution. The 2D and 1D variants are much more practical from an implementation perspective, while retaining all the benefits. We believe the Beam Beam 1D estimate (Eq. (38)) and Beam Beam 2D1 estimate (Eq. (29)) are the most promising choices. The remaining results focus on the 1D version because it retains the benefits of a Beam Beam estimate and is computationally simpler to evaluate. Figure 10 illustrates the ability of photon beams to ef ciently handle area light sources. We render an artifact-free image with just 5k photon beams, whereas using 25k photon points in roughly equal time results in over-blurring (near the light source) and no visible volumetric shadow from the tall box. With 10M photon points, overblurring artifacts vanish, but the render time is over 16 times longer and variance is still present in the estimate, resulting in a slightly bumpy appearance. Note also that our use of photon differentials works well, even for area light sources, to adaptively blur the photon beams based on the expected local density. The benefis of photon beams for multiple scattering are clearly visible in Figure 13. In Figure 11 we perform a quantitative convergence comparison between photon points and photon beams. Unfortunately, there are currently no algorithms which can generate unbiased reference solutions for general caustics from point lights. To circumvent this problem we constructed a focused lighting feature similar to a volume caustic (but which has a readily computable reference solution) using single scattering from a Phong-light with a high exponent. The reference solution was computed using ray marching with a very small step-size. We plot the RMS error, on a log-log scale, when reconstructing using photon beams and photon points. We also compute and plot the perceptual error based on the structural similarity index (SSIM) [Wang et al. 2004]. Both graphs show that we would need between 1M and 10M photon points to obtain the same error as just 100 photon beams, an improvement of over 4 orders of magnitude. These photons not only need to be traced, but also stored explicitly in the photon map. Photon beams provide a bene t in both execution time and storage requirements since they are a more compact representation of the photon tracing stage. For the remaining scenes, we compute ground truth images with volumetric photon mapping using billions of photons. To obtain reference quality results, we set the projected radius of each photon to be smaller than a pixel (to minimizes bias), and, to avoid the A Comprehensive Theory of Volumetric Radiance Estimation ¢ Fig. 12. A progression of renderings of a glass sphere forming a volume caustic. We compare the convergence of radiance estimation using photon beams (left half), and photon points (right half of each image). In each image pair we compare photon beams to a result generated using 16 as many photon points. Photon beams achieve higher quality in less time than photon points, even when using a signicantly smaller number of beams than points. Fig. 13. Cornell box with an area light and multiple scattering. Left to right: photon beams, photon points with roughly equal-time and equal-quality. memory requirements for storing billions of photons, we compute several hundred independent renderings and average the results. We include the averaged, converged results as ground truth and report RMSE and SSIM values for each image. In Figure 12 we show a sequence of renderings of a volume caustic from a glass sphere (inspired by the scene from Jensen and Christensen [1998]) to examine the qualitative and quantitative convergence behavior of photon beams and photon points. We increase the number of photons from left to right and split each image vertically to compare beams and points. Each split image compares photon beams to 16 times as many photon points. We provide a ground truth image for comparison (using around 1 billion effective photon points) and report RMSE and SSIM values. Even with 16 times as many points as beams, in each case, using photon beams is faster and results in less blur and noise. Photon beams are a perfect representation for this type of focused illumination and can therefore converge to a crisp and noise-free solution using only 32k beams in 43 seconds, whereas even with 512k photons and almost twice the time, the caustic is over-blurred and noisy. In Figure 8 we showed the improvement in reconstruction for this scene due to using an adaptive beam width based on photon differentials. Fixed-width beams result in over-blurring and banding, whereas differentials allow for a small beam width in the focused regions of the volume caustic and avoid potential banding issues in sparse regions. Figure 14 replicates the bumpy sphere scene from Walter et al. [2009] rendered with photon beams, photon points, and Walter ™s direct algorithm. We also provide a ground truth image rendered using 2.5 billion photons. In about 100 seconds, we are able to reconstruct extremely sharp lighting features using only 90k photon beams while with the same number of photon points, the BRE cannot resolve these features, suffering from over-blurring. Given equal render time, we can render the scene using 1.3M photon points; however, the resulting image still cannot resolve the small features and suffers from a bumpy or noisy appearance. Moreover, when using photon points, the stochastic photon tracing process results in distracting ‚ickering as this scene is animated, whereas this ‚ickering is not present when using photon beams (see the accompanying video). The last image was kindly provided by Bruce Walter and rendered using his and his colleagues ™ approach [Walter et al. 2009]. One of the strengths of this method is that it directly solves for valid refracted light paths without the need for an intermediate lighting representation, so it does not suffer from blurring and can resolve ne details not possible with standard photon mapping. This image was rendered in a different renderer, and timing was measured on an 8-core, 2.83 Ghz machine with dual Intel Xeon 5440 chips. We see that Walter ™s method is about 3 times as slow rendering on 8 cores compared to photon beams on only one core. Though part of this performance penalty could be attributed to differences in implementation framework, this great disparity also suggests algorithmic factors in performance. Part of this could be due to Walter ™s method requiring ray marching to solve for the accumulated in-scattered radiance along a ray (so a small amount of noise is still visible) whereas our method solves for the beam radiance explicitly without the need for ray marching, resulting in a completely noise-free solution. Walter ™s method also has a number of practical limitations. The method is currently restricted to a single specular bounce, so while it can handle a single refraction in this bumpy sphere scene, it could not render a volume caustic from a sphere as seen in Figure 12. Even in the bumpy sphere scene, there are valid light paths which refract more than once within the medium due to total internal re ‚ection, which we take into account with photon beams, but which Walter ™s method cannot simulate. Note also that although Walter ™s method is in theory unbiased, in practice it requires clamping to avoid bright pixels, which can lead ¢ W. Jarosz et al. Fig. 14. An equal-time, and equal number-of-photons comparison of photon points and photon beams in the bumpy sphere test scene [Walter et al. 2009]. Photon beams accurately capture complex focusing patterns within the sphere with only 90k beams. The same number of photon points renders faster but with excessive blurring. At 1.3M photon points (equal time), blurring and variance still remain. We also provide a ground truth image (left) computed using several billion photons. For reference, we also compare our result with a rendering using Walter ™s method (right), which takes 3 times as long to render ( — using a different implementation on a machine with 8 cores), and still contains noise due to the need for ray marching. Fig. 15. A re-creation of the lighthouse scene from Jarosz et al. [2008]. Using photon beams we can render a higher quality image, using fewer photons and in less time when compared to the beam radiance estimate using photon points. Fig. 16. False-color visualizations of the per-pixel structural similarity index for equal-time renderings of the bumpy sphere and lighthouse scenes. to an arbitrary amount of energy loss. Our method does not require such clamping. Lastly, Walter ™s method only operates on triangle meshes (admittedly a fairly reasonable restriction). Photon beams can be used with any arbitrary surface representation, including implicit surfaces and procedural geometry. In Figure 15 we show a frame from an animated re-creation of the lighthouse scene [Jarosz et al. 2008]. In 25 seconds we render a frame with no artifacts using 700 photon beams, while even with 10k photon points and slightly greater render time, low-frequency noise is clearly visible, which also ‚ickers when animated. Figure 16 visualizes false-color per-pixel structural similarity for the bumpy sphere and lighthouse scenes and reports RMSE and SSIM values. Photon points have dif culty reconstructing sharp lighting features like the start of the lighthouse beam, or the intricate light patterns inside the bumpy sphere, resulting in noticeable error. Finally, Figure 17 visualizes underwater beams of light refracting through an animated ocean water surface. The ground truth image required around 8 billion photon points for a converged result. Here, the sun beams are much more ef ciently represented with photon beams, resulting in a more faithful, noise-free reconstruction without temporal ‚ickering. In contrast, with an equal-time photon point rendering, the individual beams of light are barely discernible. The accompanying video shows an animated comparison. 

10. DISCUSSION 

In addition to providing a practical rendering algorithm for participating media, the radiance estimates we present also offer interesting insights into connections between otherwise fairly distinct rendering algorithms. In this section we discuss how photon beams can be seen as a generalization of a number of other algorithms. 10.1 Generalization of Photon Mapping Photon beams can clearly be seen as a generalization of standard photon mapping where a more compact data representation is used to connect the photon tracing and radiance estimation stages. Our theory of photon beams can also be interpreted as a generalization A Comprehensive Theory of Volumetric Radiance Estimation ¢ Fig. 17. An animation frame of underwater sun rays refracting through an ocean surface. Photon beams (left) accurately reconstruct this lighting with only 25k beams. In equal time, 100k photon points (middle) could be stored, but the resulting reconstruction suffers from noisy and over-blurred lighting features. of the ray mapping algorithm [Lastra et al. 2002; Havran et al. 2005; Herzog et al. 2007] to participating media. This suggests the possibility of applying photon beams to other related problems where photon maps and ray maps have been applied, including the rendering of hair [Moon and Marschner 2006] and subsurface scattering [Jensen 2001]. Moreover, a single collection of photon beams can be used to estimate lighting within media using our estimators, and lighting on surfaces using ray mapping density estimation. This eliminates the need to store distinct representations for these two types of lighting simulations. 10.2 Generalization of Beam Tracing Methods Light beam tracing [Watt 1990; Iwasaki et al. 2001; Ernst et al. 2005] simulates re ‚ective or refractive caustics by extruding beams or prisms by the bending light at vertices of a triangle mesh. This has been applied to participating media as well [Nishita and Nakamae 1994], and in fact the computation of in-scattered radiance with these methods is nearly mathematically identical to the radiance estimates in Eqs. (29) and (33). In this sense, photon beams can be viewed as a generalization of beam tracing methods, where the beam representation is no longer dictated by the geometric representation. Instead, emitted photons implicitly de ne beams, whereas mesh triangles de nes beams/prisms in beam tracing methods. This is a significant bene t since photon beam complexity depends on induced radiometric complexity, not geometric complexity. regions of the scene. In this interpretation, the contribution of each photon or VPL includes an inverse-squared distance term which needs to be clamped to avoid singularities near the VPL. In contrast, in the density estimation interpretation, this clamping is not necessary and each photon ™s contribution is spread within a small spatial neighborhood around the photon, inducing a blurring of the illumination. The generalization of photon points to photon beams provides another way to arrive at this dual interpretation. With photon beams, much like VPL methods, a photon ™s power contributes to a large region in space, potentially far away from the start of the beam. In effect, the point at the start of the photon beam is a photon point (see Figure 1) that illuminates the scene with a beam of light in a particular direction. Furthermore, if we use a conical blur around the beam with the estimate in Eq. (25), Li (x ’ ) ˆ b ˆRb f ( b ) b e ’ t tx,b , 2 R (tx,b ) (39) 10.3 Generalization of Shadow Maps Photon beams can also be viewed as a direct generalization of shadow maps in participating media. This connection can be seen by observing individual beams in Figure 3. When simulating single scattering from a spot-light, photon beams contain the same information as a shadow map “ the visibility from surface points back to the light source. If the photons were emitted on a regular grid from the spot-light, then these two data representations would have a one-to-one mapping: each photon beam corresponds to a single shadow map pixel. Hence, for situations where a shadow map could be used, we could treat the shadow map as an implicit representation of the photon beams and, without any acceleration structure, could perform camera ray intersection with all shadow map beams by stepping through the shadow map pixel grid. Note, however, that photon beams are more ‚exible than shadow maps since they allow for area light sources and multiple specular and diffuse bounces. 10.4 Connection to VPL-Methods and Final Gather Virtual point-light methods such as Instant Radiosity [Keller 1997] and Lightcuts [Walter et al. 2006] interpret a photon map ™s photons as point-light sources that illuminate other, potentially distant, we re-introduce the familiar inverse-squared falloff term present in VPL methods since the cross-section of the cone goes to zero as we approach the start of the photon beam. Note that this radiance estimate is identical to looping over a collection of VPLs with power b . Hence, by simulating each photon beam as a photon œcone , photon beams effectively compute a VPL solution where the VPLs are beam- or spot-lights, and would obtain mathematically similar results to unclamped VPL methods for participating media. The connection to VPL methods goes even further. The main motivation for Lightcuts is to reduce the number of shadow rays necessary to evaluate lighting from VPLs. Given that Eq. (39) is effectively a VPL estimate, we can further note that it actually requires no shadow rays. The visibility information established during the photon tracing stage is re-used within each beams ™ search region during radiance estimation, thereby eliminating the need for any shadow rays. Photon beams can therefore be thought of as a form of Instant Radiosity without the need for any shadow rays, plus with the added ability to estimate the integrated in-scattered radiance along the length of an entire camera ray. Recently, HaË‡an et al. [2009] introduced the concept of the virtual s sphere-light (VSL) as a way to avoid the problems with the inverse squared falloff term in surface-based VPL methods. Essentially, each VSL ™s energy is spread across a nite area on nearby surfaces by expanding a sphere, and the corresponding energy œsplat is treated as an area light source when evaluating the indirect illumination at other scene locations. The singularity is avoided since the lighting contribution is integrated over a nite solid angle. We implicitly bene t from this same enhancement with the photon beams framework. With a conical blur region, the beam width falls off to zero at the start of the beam and induces an in nite radiance; however, the cross-section at the start of a beam need not be zero. In our implementation, beams emitted from area lights have a ¢ W. Jarosz et al. kernel with photon beams. This is a challenging geometric problem since an appropriate de nition of œnearest would need to be derived. nite cross-section and, for secondary scattering, we use the photon differentials ™ footprint to compute the cross-section at the start of the child beam. This induces a conical frustum shape and avoids the division by zero. Hence, photon beams can be seen as a generalization of VPLs where the clamping is avoided by always forcing nonzero starting cross-sections (replacing potentially unbounded energy loss with blurring), and they allow for computing the beam radiance directly using the Beam Beam estimates (without the need for ray marching). Finally, using similar reasoning, the photon beam radiance estimates can also be interpreted as a selective form of nal gathering. For instance, Eqs. (25) and (39) compute the value of in-scattered radiance at a point x by examining all photon points (beam start points) which œsend energy towards x. This could instead have been computed by tracing nal gather rays out from x to nd these photons. However, as mentioned above, in contrast to nal gathering or VPL methods, the beam radiance estimate does not require any ray intersections with the scene to determine visibility. The visibility between the query location and all photon points is encoded in the beams themselves. Furthermore, the Beam Beam radiance estimates effectively compute nal gathering from all points along the length of a camera ray without the need for ray marching. 

12. CONCLUSION 

11. LIMITATIONS & FUTURE WORK 

The connections discussed above suggest a number of interesting avenues of future work. In addition, our prototype implementation, though effective, does still have a number of limitations which provides opportunities for further improvement and investigation. We have developed a novel, comprehensive theory of volumetric radiance estimation. This theory allows for estimating in-scattered radiance at a point, or the accumulated in-scattered radiance along a camera beam. Both of these operations can be performed using the standard photon map representation as done in previous work. Moreover, our theory generalizes both of these operations by introducing the concept of photon beams, a more compact, and expressive intermediate representation of lighting in participating media. The combination of these two data representations and two query operations results in a collection of nine distinct radiance estimates for computing complex lighting in participating media. Due to the increased expressiveness of this new lighting representation, photon beams have a significant performance and quality bene t over standard volumetric photon mapping, while requiring less photons. This representation can also be viewed as a way to implicitly increase the effective œ resolution of a photon map, thereby reducing bias (blur) and also significantly reducing variance. Using this representation, we are able to render extremely sharp details (such as volume caustics) using just tens of thousands of photon beams, whereas this would have required millions or billions of photons points with previous methods